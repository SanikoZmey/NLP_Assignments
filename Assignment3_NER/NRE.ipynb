{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Alexander Kurdyukov BS21-AI-01`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For training the toc2vec model `ru_core_news_lg` vectors were used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download ru_core_news_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing needed packages and loading the HuggingFace dataset to use its part as test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ario\\miniconda3\\envs\\InfoRetr\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the latest cached version of the module from C:\\Users\\Ario\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\MalakhovIlya--RuNNE\\5c0467600cde2a64546227a05688adcfcdcb583c442a7cac64b864313a68e588 (last modified on Thu Apr 11 20:23:35 2024) since it couldn't be found locally at MalakhovIlya/RuNNE, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "spacy.require_gpu()\n",
    "\n",
    "dataset = load_dataset('MalakhovIlya/RuNNE', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function for processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "def get_spacy_doc(data):\n",
    "  ### Create a blank spaCy pipeline\n",
    "  nlp = spacy.blank('ru')\n",
    "  \n",
    "  ### Create DocBin object to store processed texts\n",
    "  db = DocBin()\n",
    "\n",
    "  ### Some counters for statistics\n",
    "  counter_all, counter_filt, counter_ign = 0, 0, 0\n",
    "\n",
    "  ### Iterate through the data\n",
    "  for line in tqdm(data):\n",
    "    doc = nlp.make_doc(line[\"sentences\"])\n",
    "    annot = line['ners']\n",
    "    ents = []\n",
    "\n",
    "    ### Extract entities from the annotations\n",
    "    for start, end, label in annot:\n",
    "      start, end = int(start), int(end)\n",
    "      try:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
    "        ### Some other attempts to get character span(due to the broken positionings)\n",
    "        if span is None:\n",
    "          span = doc.char_span(start, end + 1, label=label, alignment_mode='strict')\n",
    "        if span is None:\n",
    "          span = doc.char_span(start - 1, end, label=label, alignment_mode='strict')\n",
    "      except:\n",
    "        continue\n",
    "\n",
    "      if span is None:\n",
    "        ### Log errors for annotations that couldn't be processed\n",
    "        # print(str(line[\"sentences\"])[start: end], doc.char_span(start, end, label=label, alignment_mode='expand'), label)\n",
    "        counter_ign += 1\n",
    "      else:\n",
    "        ents.append(span)\n",
    "      counter_all += 1\n",
    "    \n",
    "    try:\n",
    "      ### Filtering overlapping and repeating spans and then saving the text and NER-data\n",
    "      filtered_ents = spacy.util.filter_spans(ents)\n",
    "      doc.ents = filtered_ents\n",
    "      counter_filt += len(filtered_ents)\n",
    "      db.add(doc)\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  ### Printing some global statistics about the spans \n",
    "  print(counter_all, counter_filt, counter_ign)\n",
    "  return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the train and test data with some formatting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/train.jsonl') as f:\n",
    "    c_train_data = [json.loads(line) for line in f]\n",
    "\n",
    "test_data = [{\"sentences\": dataset['test'][\"text\"][line_id], \"ners\": [triple.split() for triple in dataset['test'][\"entities\"][line_id]]} for line_id in range(len(dataset['test']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing all the data and creating data files for spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ario\\miniconda3\\envs\\InfoRetr\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\Ario\\miniconda3\\envs\\InfoRetr\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "100%|██████████| 519/519 [00:04<00:00, 129.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30835 23963 227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [00:00<00:00, 95.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5843 4605 30\n"
     ]
    }
   ],
   "source": [
    "train_db = get_spacy_doc(c_train_data)\n",
    "test_db = get_spacy_doc(test_data)\n",
    "\n",
    "train_db.to_disk(\"train_data.spacy\")\n",
    "test_db.to_disk(\"test_data.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tok2Vec model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config_tok2vec.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config_tok2vec.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ario\\miniconda3\\envs\\InfoRetr\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\Ario\\miniconda3\\envs\\InfoRetr\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config base_config_tok2vec.cfg config_tok2vec.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainging tok2vec model for 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: .\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00    152.76    1.35    0.79    4.47    0.01\n",
      "  0     200       1543.45  14883.35   34.71   42.27   29.45    0.35\n",
      "  0     400        892.28  10505.61   51.55   58.94   45.80    0.52\n",
      "  1     600       1873.34   9459.88   55.96   57.66   54.35    0.56\n",
      "  1     800        478.80   7769.56   58.08   59.17   57.02    0.58\n",
      "  1    1000       1333.20   8323.12   60.80   62.59   59.11    0.61\n",
      "  2    1200       1422.10   6718.21   59.77   63.19   56.70    0.60\n",
      "  2    1400        501.70   6666.92   59.24   65.15   54.31    0.59\n",
      "  3    1600        565.10   5965.39   61.70   65.93   57.98    0.62\n",
      "  3    1800        576.40   4938.65   63.92   66.05   61.93    0.64\n",
      "  3    2000        741.46   5647.63   62.86   66.51   59.59    0.63\n",
      "  4    2200        501.47   4371.25   64.15   69.61   59.48    0.64\n",
      "  4    2400       2534.88   4599.68   63.76   64.64   62.91    0.64\n",
      "  5    2600        588.41   4523.81   63.55   64.95   62.21    0.64\n",
      "  5    2800       1703.35   3942.92   62.41   63.02   61.80    0.62\n",
      "  5    3000        668.41   3814.82   63.88   64.83   62.95    0.64\n",
      "  6    3200        962.37   3415.21   63.48   64.91   62.11    0.63\n",
      "  6    3400        658.76   3292.61   65.07   67.11   63.15    0.65\n",
      "  6    3600       1126.92   3247.26   64.70   68.48   61.32    0.65\n",
      "  7    3800        671.26   2926.48   64.15   66.11   62.30    0.64\n",
      "  7    4000       2095.63   2890.98   64.45   67.64   61.54    0.64\n",
      "  8    4200        782.93   3040.47   64.71   65.28   64.15    0.65\n",
      "  8    4400        734.77   2472.45   64.21   65.03   63.41    0.64\n",
      "  8    4600        722.03   2658.46   63.28   65.61   61.11    0.63\n",
      "  9    4800        710.51   2203.31   64.07   65.39   62.80    0.64\n",
      "  9    5000        715.35   2498.91   63.77   67.03   60.80    0.64\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ario\\miniconda3\\envs\\InfoRetr\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\Ario\\miniconda3\\envs\\InfoRetr\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train --gpu-id 0 config_tok2vec.cfg --output ./ --paths.train ./train_data.spacy --paths.dev ./test_data.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init fill-config base_config_trans.cfg config_trans.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainging transformer model for 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: .\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ario\\miniconda3\\envs\\InfoRetr\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\Ario\\miniconda3\\envs\\InfoRetr\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0         808.42    820.40    0.12    0.08    0.26    0.00\n",
      "  2     200      806782.36  173838.81   37.19   62.23   26.51    0.37\n",
      "  5     400       89629.25  58944.69   73.55   73.83   73.27    0.74\n",
      "  7     600       20693.63  33460.93   77.81   77.16   78.48    0.78\n",
      " 10     800        7900.66  23606.74   78.06   77.68   78.44    0.78\n",
      " 12    1000        5990.45  19744.57   78.06   75.80   80.46    0.78\n",
      " 15    1200        2786.91  17490.43   78.31   76.53   80.17    0.78\n",
      " 17    1400        2571.47  16501.79   77.54   75.10   80.15    0.78\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train --gpu-id 0 config_trans.cfg --output ./ --paths.train ./train_data.spacy --paths.dev ./test_data.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and testing the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ario\\miniconda3\\envs\\InfoRetr\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "tok2vec_model = spacy.load(\"model-best\")\n",
    "transformer_model = spacy.load(\"prev_best_trans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('data/dev.jsonl') as f:\n",
    "    dev_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the model on sentences from the set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Путин PERSON\n",
      "закона Димы Яковлева LAW\n",
      "Президент России PROFESSION\n",
      "Владимир Путин PERSON\n",
      "закона Димы Яковлева LAW\n",
      "Советом Федерации ORGANIZATION\n",
      "в четверг 27 декабря DATE\n",
      "Владимир Путин PERSON\n",
      "О мерах воздействия на лиц LAW\n",
      "Димы Яковлева ORGANIZATION\n",
      "президент РФ PROFESSION\n",
      "заседании EVENT\n",
      "Госсовета ORGANIZATION\n",
      "Путина PERSON\n",
      "американские власти ORGANIZATION\n",
      "Российский Президент ORGANIZATION\n",
      "российских COUNTRY\n",
      "Госдумой ORGANIZATION\n",
      "Димы Яковлева ORGANIZATION\n",
      "американцам NATIONALITY\n",
      "России COUNTRY\n",
      "Советом Федерации ORGANIZATION\n",
      "российских законодателей ORGANIZATION\n",
      "Конгрессом США ORGANIZATION\n",
      "Россией COUNTRY\n"
     ]
    }
   ],
   "source": [
    "text = dev_data[5]['senences']\n",
    "# print(text)\n",
    "\n",
    "out = tok2vec_model(text)\n",
    "for ent in out.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Путин PERSON\n",
      "закона Димы Яковлева LAW\n",
      "Президент России PROFESSION\n",
      "Владимир Путин PERSON\n",
      "закона Димы Яковлева LAW\n",
      "Советом Федерации ORGANIZATION\n",
      "в четверг 27 декабря DATE\n",
      "Владимир Путин PERSON\n",
      "О мерах воздействия на лиц, причастных к нарушениям основополагающих прав и свобод человека, прав и свобод граждан РФ LAW\n",
      "закон Димы Яковлева LAW\n",
      "президент РФ PROFESSION\n",
      "заседании EVENT\n",
      "Госсовета ORGANIZATION\n",
      "Путина PERSON\n",
      "американские власти ORGANIZATION\n",
      "Российский Президент PROFESSION\n",
      "российских COUNTRY\n",
      "тяжелых заболеваний DISEASE\n",
      "Госдумой ORGANIZATION\n",
      "закон Димы Яковлева LAW\n",
      "американцам NATIONALITY\n",
      "России COUNTRY\n",
      "Советом Федерации ORGANIZATION\n",
      "российских COUNTRY\n",
      "законодателей PROFESSION\n",
      "Конгрессом США ORGANIZATION\n",
      "Закон о нормализации торговых отношений с Россией, LAW\n",
      "закон Магнитского LAW\n"
     ]
    }
   ],
   "source": [
    "text = dev_data[5]['senences']\n",
    "# print(text)\n",
    "\n",
    "out = transformer_model(text)\n",
    "for ent in out.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model to get NERS from all the sentences of the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:04<00:00, 14.95it/s]\n"
     ]
    }
   ],
   "source": [
    "out_data = []\n",
    "for line in tqdm(dev_data, total=len(dev_data)):\n",
    "    out = model(line[\"senences\"])\n",
    "    ents = []\n",
    "    for ent in out.ents:\n",
    "        ents.append([ent.start_char, ent.end_char, ent.label_])\n",
    "    out_data.append({\"id\":line[\"id\"], \"ners\": ents})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results in `test.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.jsonl\", 'w') as f:\n",
    "    for item in out_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('test.jsonl') as f:\n",
    "#     data = [json.loads(line) for line in f]\n",
    "\n",
    "# data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "InfoRetr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
