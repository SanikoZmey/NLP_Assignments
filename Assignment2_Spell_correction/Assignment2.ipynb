{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `Alexander Kurdyukov BS21-AI-01`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvig’s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing all necessary packages\n",
        "In this implementation it was decided to use already codded spell corrector due to the fact that otherwise it was needed to write(indeed copy already exisiting) solution and in order to reduce amount of code to check(which anyways would have been just copied) and to not reinvent a wheel the corrector with the same edit1 and edit2 functions implemented will be used as python package. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [],
      "source": [
        "from autocorrect import Speller\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "spell = Speller()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing dictionary with bigram counts, counting total number of bigrams, and saving second grams from pairs\n",
        "The process is just loading already preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading bigrams.txt: 100%|██████████| 1020385/1020385 [01:21<00:00, 12591.54it/s]\n",
            "Loading coca_all_links.txt: 100%|██████████| 156306/156306 [00:11<00:00, 13158.84it/s]\n"
          ]
        }
      ],
      "source": [
        "bigr_counts = {}            ## Dictionary with counts for each starting gram its complementary second one\n",
        "sec_bigr_vocab = set()      ## Set of all grams from that are complementary ones \n",
        "total_count = 0\n",
        "\n",
        "## I used 2 prvided datasets(for test and demonstrational purposes I hope it is OK to have only them)\n",
        "for dataset_path in [\"bigrams.txt\", \"coca_all_links.txt\"]:\n",
        "    df = pd.read_csv(dataset_path, sep='\\t', names=[\"counts\", \"first_gram\", \"second_gram\"], usecols=[0, 1, 2], encoding='latin-1')\n",
        "    df[[\"first_gram\", \"second_gram\"]] = df[[\"first_gram\", \"second_gram\"]].astype(str)\n",
        "    total_count += np.sum(df['counts'].astype(np.int64).values)\n",
        "    bar = tqdm(df.iterrows(), total=df.shape[0], desc=f\"Loading {dataset_path}\")\n",
        "    for index, row in bar:\n",
        "        count, f_gram, s_gram  = row[\"counts\"], row[\"first_gram\"].lower(), row[\"second_gram\"].lower()\n",
        "        if(f_gram not in bigr_counts):\n",
        "            bigr_counts[f_gram] = {s_gram: count}\n",
        "        elif(s_gram not in bigr_counts[f_gram]):\n",
        "            bigr_counts[f_gram][s_gram] = count\n",
        "        else:\n",
        "            bigr_counts[f_gram][s_gram] += count\n",
        "\n",
        "        sec_bigr_vocab.add(s_gram)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Next, there is code for context sensitve spell correction.  \n",
        "The implementation was almost not inspired by any open source code in the Internet and hence is in the \"testing\" and \"working in theory\" states. It may not be optimised(both in space and exec. time) well and probably performs poor(but it was tried to implement it not to be so)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['we', 'will', 'be', 'doing', 'sport', 'and', 'you', 'need', 'to', 'abandon', 'their', 'dying', 'man']\n"
          ]
        }
      ],
      "source": [
        "def calc_prob(cur_gram: str, pred_gram: str, found_probs: dict, depth: int, cur_depth: int = 0):\n",
        "    \"\"\"\n",
        "        Function for calculating prbabilities to construct N-grams from bigrams of curtain N(depth/width)\n",
        "    \"\"\"\n",
        "    if((cur_gram, pred_gram, depth) in found_probs):            ## If this chain was already computed earlier \n",
        "        return found_probs[(cur_gram, pred_gram, depth)]        ## then we return the stored result\n",
        "\n",
        "    if(cur_gram in bigr_counts):                                        ## If the current gram starts any known bigram\n",
        "        all_count = sum(bigr_counts[cur_gram].values())                 ## then we sum up all the counts of bigrams that can be constructed and are known\n",
        "        if(cur_depth + 1 == depth):                                     ## If the needed depth is reached \n",
        "            gr_count = bigr_counts[cur_gram].get(pred_gram, -1.0)       ## then we get counts of bigram that is constructed from the current gram and the gram that is supposed to end the N-gram\n",
        "            if(gr_count != -1):                                         ## If their are such known bigrams \n",
        "                to_sum = gr_count / all_count                           ## then the probability of constructing it is following: number of needed bigrams / number of all bigrams both of which can be constructed from the current gram\n",
        "            else:\n",
        "                to_sum = 1 / (all_count + len(bigr_counts[cur_gram]))   ## Otherwise, we suppose that there we addeded len(bigr_counts[cur_gram]) new bigrams to the dictionary and 1 of them is the needed one\n",
        "            return to_sum                                               ## And after that we return the probability\n",
        "        else:\n",
        "            prob_sum = 0.0                                              ## If the needed depth was not reached \n",
        "            for gram in bigr_counts[cur_gram].items():                  ## Then we iterate through all the grams that are complementary to the current one \n",
        "                prob_sum += (gram[1] / all_count * calc_prob(gram[0], pred_gram, found_probs, depth, cur_depth+1))   ## And calculate the probability as: probability to pick the gram as complementary * probability to pick the next complpementary gram\n",
        "            found_probs[(cur_gram, pred_gram, depth)] = prob_sum                                                     ## In such a way we calculate the probability of constructing an N-gram that starts and ends with needed grams\n",
        "            # print(prob_sum, cur_gram, pred_gram)\n",
        "            return prob_sum     ## Do not forget to return probabilities in all cases\n",
        "    return 1 / total_count      ## If from the chain breaks from the very start - return some small value\n",
        "\n",
        "def fix(text: list, beam_width: int = 3, wind_size: int = 3):\n",
        "    \"\"\"\n",
        "        Function that fixes the given text with the use of bigram and word frequencies \n",
        "    \"\"\"\n",
        "    found_probs = {}    ## Dictionary for memorizing already checked grams chains with curtain depth/width\n",
        "    fixed = [[]]    ## List of all generated versions/fixes of the passed text\n",
        "    poses = []      ## Positions of the words that are classified by the algorithm as \"words with typos\"\n",
        "    probs = [0.0]   ## Log10(P)'s of the versions/fixes - without \"log10\" just probabilities to fix the text in a curtain way\n",
        "    \n",
        "    ## Iterating through all the words/tokens/grams in the text\n",
        "    for id, word in enumerate(text):\n",
        "        ## Getting candidates for each word/token/gram and sorting them by frequency\n",
        "        candidates = sorted([cand for cand in spell.get_candidates(word) if (cand[1] in bigr_counts or cand[1] in sec_bigr_vocab)], key=lambda pair: pair[0], reverse=True)\n",
        "        \n",
        "        ## If word is \"known\" then we do not need to change/fix it\n",
        "        if(word in bigr_counts or word in sec_bigr_vocab or len(candidates) == 0):\n",
        "            fixed = [fix+[word] for fix in fixed]\n",
        "        else: \n",
        "            candidates = [cand for cand in candidates[:beam_width]]                         ## Otherwise, we take top-beam_width most frequent words\n",
        "            # print(candidates)\n",
        "            new_fixes = []                                                                  ## Prepare spaces for the new versions\n",
        "            new_probs = []                                                                  ## and probabilities\n",
        "            for f_id, fix in enumerate(fixed):                                              ## Iterating over all versions\n",
        "                for cand in candidates:                                                     ## and adding to each version/fix each candidate word\n",
        "                    new_fixes.append(fix+[cand[1]])                                         ## Save each version/fix\n",
        "                    new_probs.append(probs[f_id] + np.log10(cand[0]/candidates[0][0]))      ## and calculate weighted and smoothed probability(idea to log10 probabilities is taken from https://www.kaggle.com/code/dhruvdeshmukh/spelling-corrector-using-n-gram-language-model)\n",
        "            fixed = new_fixes                                                               ## Replace old versions/fixes with new ones\n",
        "            probs = new_probs                                                               ## the same with probababilities\n",
        "            poses.append(id)                                                                ## and add the position of the words we change/fix\n",
        "    \n",
        "    ## After all the versions are generated we can try to calculate which version is most \"appropriate\" according to the context of the original text\n",
        "    half_size = (wind_size - 1) // 2 + 1                                                                ## Firstly, we calculate max size of the N-grams which we will use in order to try to consider context of the oroginal text(all the wind_size's are odd numbers are odd and > 1... just for simplicity)\n",
        "    for f_id, fix in enumerate(fixed):                                                                  ## Iterate over all the versions/fixes\n",
        "        for pos in poses:                                                                               ## Iterate over all the positions of words to be changed/fixed \n",
        "            for neigh_diff in range(1, half_size):                                                      ## Iterate over all the Ks for K-grams constructions\n",
        "                if(pos - neigh_diff > -1):                                                              ## If we not overcome the start of the text\n",
        "                    probs[f_id] += np.log10(calc_prob(fix[pos - neigh_diff], fix[pos], found_probs, neigh_diff))    ## Then we compute smoothed probability of the fixed word to be at the end of gram with lengh = neigh_diff\n",
        "                if(pos + neigh_diff < len(fix)):                                                                    ## If we not overcome the end of the text\n",
        "                    probs[f_id] += np.log10(calc_prob(fix[pos], fix[pos + neigh_diff], found_probs, neigh_diff))    ## Then we compute smoothed probability of the fixed word to be at the beginning of gram with lengh = neigh_diff\n",
        "    \n",
        "    \"\"\" Some debugging info\"\"\"\n",
        "    # print(fixed)\n",
        "    # print(probs)\n",
        "    # print(poses)\n",
        "    return fixed[np.argmax(probs)]  ## At the very end we return the most probale variant of the changed/fixed text accordind to the algorithm's logic\n",
        "\n",
        "test = \"we wll be dking sport and yow need to abandon their dking man\"\n",
        "result = fix(test.split(), 3, 5)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "`Due to 2 reasons 2 bigram datasets were used:` \n",
        "1) Whole solution was designed to be a test of theory and hence there was no need in large datasets since in such case it was much harder to debug the process. \n",
        "2) Among all provided data sets 2 with bigrams were rather large to then use quite large variety of words for test sentences creation.\n",
        "3) In theory all N-grams(with N > 2) can be divided on bigrams with additional computational steps\n",
        "\n",
        "`If the word/gram was in the beginning or middle of a N-gram then its probability is 1 / sum of all counts of bigrams(can be changed to number of known distinc words). If the word/gram is in the end of a N-gram then its probability is 1 / sum of counts of known bigrams that can be constructed from the preceding gram.`\n",
        "\n",
        "`Empirically it was found that values 3 and 4 for a width of a beam are rather effective in a process of spell correction.`\n",
        "\n",
        "`Window size(or N) for N-grams that are used in considering context are odd numbers > 1: 3, 5 etc. due to simplicity and symmetry property`\n",
        "\n",
        "#### `All additional justifications can be found as comments to the code or to the headers of cells`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Next cell is just a copy of [Norvig's solution](https://norvig.com/spell-correct.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words(open('big.txt').read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Since the main task is to create contex-sensitive N-gram spell corrector and not to write \"smart\" text tokenizer it was decided to write simple regex tokenizer(that partially is Norvig's code) and to preprocess the text by hands(sentence separation).\n",
        "In addition to that, for the test/demonstration of algorithms there will be no automatically generated errors (like [here](https://www.kaggle.com/code/dhruvdeshmukh/spelling-corrector-using-n-gram-language-model)) and all errors will be \"made\" by hands. It was found that it is enough to see all the advantages/problems that are already known and expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['is', 'not', 'it', 'obcius', 'for', 'you', 'that', 'he', 'just', 'dooled', 'both', 'of', 'us'], ['ok', 'i', 'understood', 'your', 'poimr', 'but', 'it', 'is', 'stoll', 'hard', 'for', 'me', 'to', 'believe', 'rhat', 'he', 'made', 'it'], ['believe', 'or', 'not', 'but', 'the', 'dact', 'is', 'still', 'ghere', 'we', 'are', 'out', 'of', 'mony', 'now'], ['we', 'wll', 'be', 'dking', 'sport', 'and', 'yow', 'need', 'to', 'abandon', 'their', 'dking', 'man']]\n",
            "Context. algo:  ['is', 'not', 'it', 'obvious', 'for', 'you', 'that', 'he', 'just', 'cooled', 'both', 'of', 'us']\n",
            "Norvig's algo:  ['is', 'not', 'it', 'obvious', 'for', 'you', 'that', 'he', 'just', 'doomed', 'both', 'of', 'us']\n",
            "\n",
            "Context. algo:  ['ok', 'i', 'understood', 'your', 'point', 'but', 'it', 'is', 'stoll', 'hard', 'for', 'me', 'to', 'believe', 'that', 'he', 'made', 'it']\n",
            "Norvig's algo:  ['ok', 'i', 'understood', 'your', 'power', 'but', 'it', 'is', 'still', 'hard', 'for', 'me', 'to', 'believe', 'that', 'he', 'made', 'it']\n",
            "\n",
            "Context. algo:  ['believe', 'or', 'not', 'but', 'the', 'fact', 'is', 'still', 'here', 'we', 'are', 'out', 'of', 'many', 'now']\n",
            "Norvig's algo:  ['believe', 'or', 'not', 'but', 'the', 'act', 'is', 'still', 'there', 'we', 'are', 'out', 'of', 'many', 'now']\n",
            "\n",
            "Context. algo:  ['we', 'will', 'be', 'doing', 'sport', 'and', 'you', 'need', 'to', 'abandon', 'their', 'dying', 'man']\n",
            "Norvig's algo:  ['we', 'all', 'be', 'king', 'sport', 'and', 'you', 'need', 'to', 'abandon', 'their', 'king', 'man']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def sep(text: list):\n",
        "    repl = [re.sub(\"n't\", \" not\", re.sub(\"can't\", \"can not\", s.lower())) for s in text]\n",
        "    return [words(s) for s in repl]\n",
        "\n",
        "### Original text: https://shakespeare.mit.edu/hamlet/full.html#:~:text=Good%20now%2C%20sit,can%20inform%20me%3F\n",
        "shakespeare_text = [\"Goof now, sit drwn, and tell me, he that lnows\",   ## Good/down/knows\n",
        "        \"Why this same cttict and most obdervant watch\",                ## strict/observant\n",
        "        \"So nightly toils the dibject of the land\",                     ## subject\n",
        "        \"And wgy such daily cast of brazen vannin\",                     ## why/cannon\n",
        "        \"And foreign mart for onplements of war\",                       ## implements\n",
        "        \"Wgy such impress of shipwrights, egose sore task\",             ## why/whose\n",
        "        \"Does not duvude the Sinsay from the week\",                     ## divide/Sunday\n",
        "        \"What mofht be toward, rhat this sweaty haste\",                 ## might/that\n",
        "        \"Doth make the nifhr joint-labourer with the day\",              ## night\n",
        "        \"Who is't that can ongorm me?\"]                                 ## inform\n",
        "\n",
        "### Original text: https://en.wikipedia.org/wiki/Natural_language_processing#:~:text=Natural%20language%20processing%20(,based)%20machine%20learning%20approaches.\n",
        "scient_text = [\"Maturak language processing (NLP) is an interdisciplinary subfield of computer scurnce and linguistics\",    ## Natural/science\n",
        "                \"It is primarily comverned with giving computers the ability to dipport and manipulate human language\",     ## concerned/support\n",
        "                \"It involves processing natural language darasets, such as text corpira or speech corpora, using either rule-based or probabilistic (i.e. statistical and, modt recently, neural network-based) machine learning approaches\"]   ## datasets/corpora/most\n",
        "\n",
        "### Randomly genrated text by the implementer of this solution\n",
        "speach_text = [\"Isn't it obcius for you that he just dooled both of us?\",                               ## obvius/fooled\n",
        "               \"Ok, I understood your poimr, but it is stoll hard for me to believe rhat he made it\",   ## point/still/that\n",
        "               \"Believe or not, but the dact is still ghere: we are out of mony now!\",                  ## fact/here/money\n",
        "               \"We wll be dking sport and yow need to abandon their dking man\"]                         ## doing/you/dying\n",
        "\n",
        "### Here just change the list of strings to test algorithms in fixing text of different style \n",
        "seped_text = sep(speach_text)\n",
        "print(seped_text)\n",
        "\n",
        "for sent in seped_text:\n",
        "    result_f = fix(sent, 3, 5)\n",
        "    result_N = [correction(word) for word in sent]\n",
        "    print(\"Context. algo: \", result_f)\n",
        "    print(\"Norvig's algo: \", result_N, end=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion:\n",
        "    There are 3 types of situations during the process of correction:\n",
        "1) The written algorithm corrects the words that are not supposed to be corrected. `Reason: small \"knowledge\" dataset`\n",
        "2) The written algorithm corrects the words in a wrong way(out of context/not as in a original text): `Reasons: small/unbalanced \"knowledge\" dataset or lack of regularizing coefficients/additional constraints to work with naturally unbalanced word frequencies. Example: \"strict\"->\"attics\" even with the fact that probs of choosing \"strict\" in most of the cases were much higher`\n",
        "3) The written algorithm corrects the words in the way as it is expected.\n",
        "\n",
        "`In comparison with Norvig's solution:`\n",
        "1) In some cases Norvig's solution performs better. `Reason: sum of reasons written above. Examples: \"Good\"->\"Goof\", \"toils\"->\"tools\"`\n",
        "2) In some cases Norvig's solution gives incorrect \"fixes\" when the written algorithm gives correct ones. `Examples: \"whose\"->\"those\", \"might\"->\"most\"`\n",
        "3) In some cases both Norvig's solution and the written algorithm give incorrect \"fixes\". `Examples: \"nlp\"->\"nl\"/\"nap\", \"datasets\"->\"darasets\", \"money\"->\"many\"`\n",
        "\n",
        "All in all, written algorithm can outperform Norvig's solution if to provide larger \"knowledge\" dataset and improve the code to work better with naturally unbalanced data.\\\n",
        "For now this solution `performs in most cases not worse` than Norvig's algorithm(but if consider the fact that Norvig's solution does not take into account a context of texts...)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
